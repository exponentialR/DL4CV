{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506a84dd-241b-4e13-9ca1-bfd600747807",
   "metadata": {},
   "source": [
    "<img src = \"https://github.com/exponentialR/DL4CV/blob/main/media/BMC_Summer_Course_Deep_Learning_for_Computer_Vision.jpg?raw=true\" alt='BMC Summer Course' width='300'/>\n",
    "\n",
    "### BMC Summer Course: Deep Learning for Computer Vision, Transfer Learning Example\n",
    "\n",
    "Author: Samuel A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be4cdf-820f-4a2a-b58f-a9ab2856358e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook will guide you through the process of building a Facial Expression Recognition (FER) model using \n",
    "a pre-trained ResNet50 model. We will be leveraging transfer learning, a powerful technique that allows us to use \n",
    "a model trained on a large dataset and adapt it to our specific task with a smaller dataset.\n",
    "\n",
    "We'll use the RAF-FACE dataset, which contains images of faces annotated with one of seven emotions: \n",
    "Surprise, Fear, Disgust, Happiness, Sadness, Anger, and Neutral. The goal is to train a model that can \n",
    "accurately classify these emotions from new images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f940809-e193-4629-a150-e36268b6b44b",
   "metadata": {},
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "Before we start, let's import all the necessary libraries. These libraries will help us load and preprocess \n",
    "the data, build and train our model, and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a93f5092-9fe3-489f-8dac-a5eb5a3a7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f34b6914-02f5-4016-bf09-a04c9ac1df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard SummaryWriter\n",
    "writer = SummaryWriter('runs/rafface-transfer-learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df719804-eb56-47e8-a558-beca538fcee3",
   "metadata": {},
   "source": [
    "### Dataset Overview and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d1833-5109-453c-82e1-3d8266d59fa5",
   "metadata": {},
   "source": [
    "\n",
    "The RAF-FACE dataset consists of thousands of facial images annotated with emotions. \n",
    "We'll start by loading the image filenames and their corresponding labels from a provided text file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73dec538-60a4-4f75-afc3-6aa5b231e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory paths\n",
    "image_dir = 'data/face'  # Update this with the actual path where your images are stored\n",
    "label_file = 'data/label.txt'  # Update this if your label file is in a different location\n",
    "\n",
    "# Load the labels\n",
    "labels_df = pd.read_csv(label_file, sep=\" \", header=None, names=[\"filename\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12011544-34fa-41a4-832d-3308e369de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset: 15339\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of Dataset: {len(os.listdir(image_dir))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697f49d2-e4ce-479c-9070-97228721e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preview:\n",
      "          filename  label    emotion\n",
      "0  train_00001.jpg      5    Sadness\n",
      "1  train_00002.jpg      5    Sadness\n",
      "2  train_00003.jpg      4  Happiness\n",
      "3  train_00004.jpg      4  Happiness\n",
      "4  train_00005.jpg      5    Sadness\n"
     ]
    }
   ],
   "source": [
    "# Map the numerical labels to corresponding emotions\n",
    "emotion_mapping = {\n",
    "    1: \"Surprise\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Disgust\",\n",
    "    4: \"Happiness\",\n",
    "    5: \"Sadness\",\n",
    "    6: \"Anger\",\n",
    "    7: \"Neutral\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to the labels\n",
    "labels_df['emotion'] = labels_df['label'].map(emotion_mapping)\n",
    "\n",
    "# Preview the data\n",
    "print(\"Data Preview:\")\n",
    "print(labels_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a4827-38c0-4ec6-a02a-ac600f71076e",
   "metadata": {},
   "source": [
    "The labels dataframe now contains the image filenames, the corresponding numeric labels, \n",
    "and the mapped emotion labels. This will help us organize and process our data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c496571-4e4c-4f9a-a395-5596f53bb89e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom Dataset Class \n",
    "As you already know that in PyTorch, we often use a custom Dataset class to handle data loading. Here, we'll define a custom Dataset class that reads images from the disk,  applies transformations, and provides labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2542152c-96b0-4a77-8925-8598a41f3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAFDataset(Dataset):\n",
    "    def __init__(self, labels_df, img_dir, transform=None):\n",
    "        self.labels_df = labels_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Modify the filename to include '_aligned'\n",
    "        base_filename = self.labels_df.iloc[idx, 0]\n",
    "        aligned_filename = base_filename.replace('.jpg', '_aligned.jpg')\n",
    "        \n",
    "        img_name = os.path.join(self.img_dir, aligned_filename)\n",
    "        image = cv2.imread(img_name)\n",
    "        \n",
    "        # Check if the image was loaded successfully\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image at path {img_name} could not be loaded. Please check if the file exists and is accessible.\")\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        label = self.labels_df.iloc[idx, 1] - 1  # Adjust label to be 0-indexed\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e434a-b5f1-4f63-8b28-78f059bef3cd",
   "metadata": {},
   "source": [
    "The RAFDataset class will load an image and its corresponding label given an index. \n",
    "It also applies any specified transformations, which we'll define shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d454d9-1b38-4314-963f-a628a6749dc6",
   "metadata": {},
   "source": [
    "### Data Transformations and Splitting the Dataset \n",
    "We'll now define the transformations to be applied to the images. \n",
    "These transformations will include resizing, normalization, and data augmentation. \n",
    "We'll then split the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de00eb05-515e-4eef-a41f-a58e7d4311dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c93aa82c-2268-4f17-9e96-c3a9933df843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the dataset into training and test sets based on filenames\n",
    "train_labels_df = labels_df[labels_df['filename'].str.startswith('train')]\n",
    "test_labels_df = labels_df[labels_df['filename'].str.startswith('test')]\n",
    "\n",
    "# Create the dataset\n",
    "# Create the dataset objects\n",
    "train_dataset = RAFDataset(train_labels_df, image_dir, transform=transform)\n",
    "test_dataset = RAFDataset(test_labels_df, image_dir, transform=transform)\n",
    "\n",
    "# If you want to create a validation set from the training data, you can split the train_dataset\n",
    "val_size = int(0.2 * len(train_dataset))  # e.g., 20% of the training data for validation\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d4c4e-d61a-4b10-859d-a640d989bbb8",
   "metadata": {},
   "source": [
    "At this point, we have defined our data transformations and created data loaders \n",
    "for the training, validation, and test sets. The data loaders will be used \n",
    "to feed the data into our model during training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17b880-6b69-410c-9034-64757241b6ad",
   "metadata": {},
   "source": [
    "### Using a Pretrained Model (ResNet50)\n",
    "Now, we'll load a pre-trained ResNet50 model. This model has been trained on the ImageNet dataset, \n",
    "which contains millions of images across thousands of classes. We'll leverage the features \n",
    "it has learned and adapt them to our facial expression recognition task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fae16532-32ad-4f6e-828c-351379744180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the ResNet50 model with pre-trained ImageNet weights\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze the layers of the ResNet50 model to retain the pre-trained weights\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4826b8-fe67-413f-9068-041cd20e3e6c",
   "metadata": {},
   "source": [
    "\n",
    "Freezing the layers ensures that the pre-trained features are not modified during training. \n",
    "We'll only train the new layers that we'll add for our specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58d90d-71b6-42f1-b467-bb48dea03b57",
   "metadata": {},
   "source": [
    "### Building the Final Model\n",
    "We need to modify the final layer of the ResNet50 model to output `7` classes \n",
    "instead of the original `1000` classes (from ImageNet). \n",
    "We'll replace the final fully connected layer with a new one that has 7 output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7226fcf0-c004-4072-bbcb-2d652528937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final layer to match the number of emotion classes\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 7),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402ea95-fe5a-4ebf-afc9-88054c7c187e",
   "metadata": {},
   "source": [
    "The new final layer consists of a fully connected layer with 512 units, \n",
    "a ReLU activation function, a dropout layer for regularization, \n",
    "and a final fully connected layer with 7 output units and a LogSoftmax activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2c5ac-21fd-484c-8089-077d2f7faa4f",
   "metadata": {},
   "source": [
    "### Setting Up Training\n",
    "Before training the model, we'll define the loss function, optimizer, \n",
    "and a function for training and validating the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64c15ecd-da57-4f87-bb62-05aadce865b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=7, bias=True)\n",
       "    (4): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29772b-914a-4ddd-b3ef-064af38295a1",
   "metadata": {},
   "source": [
    "The model will use CrossEntropyLoss as the loss function and Adam as the optimizer. \n",
    "We also ensure that the model runs on a GPU if one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c2841a6-f8f3-4fab-8307-b150539eb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard - Add model graph to TensorBoard\n",
    "images, labels = next(iter(train_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "writer.add_graph(resnet, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2b094-aa0f-4cc8-9b45-a9790d0f5745",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "We will now define the functions for training and validating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84ac37ed-d736-4175-924f-4c9c176c5b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and Validation Functions\n",
    "\n",
    "\"\"\"\n",
    "We will now define the functions for training and validating the model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc = correct.double() / len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # Log the losses and accuracy to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274fb42-647d-44f5-b871-9974c46855ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard\n",
    "%tensorboard --logdir=runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677ed11-aea8-407f-851d-033c172330e3",
   "metadata": {},
   "source": [
    "The train_model function will handle both training and validation for each epoch. \n",
    "After each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb150c63-f7b2-4d57-a745-7a318fcec8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_model(resnet, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ff2d6-c469-471d-b4f5-39d855672a63",
   "metadata": {},
   "source": [
    "### 10. Evaluating the Model\n",
    "Once training is complete, we'll evaluate the model on the test set to see how well it generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0103a-6cb1-493a-ac46-2c7189b636dc",
   "metadata": {},
   "source": [
    "The evaluate_model function will compute the test loss and accuracy. \n",
    "It also stores all the predictions and true labels for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e92498-b7e1-480b-a10d-098c2a439053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    accuracy = correct.double() / len(test_loader.dataset)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Log the test loss and accuracy to TensorBoard\n",
    "    writer.add_scalar('Loss/test', test_loss)\n",
    "    writer.add_scalar('Accuracy/test', accuracy)\n",
    "    \n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47644ab0-b3dd-4e4d-8c60-e0df3f3a1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and log the results\n",
    "all_labels, all_preds = evaluate_model(resnet, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52565e-574d-4f0a-924d-2706cbf5d807",
   "metadata": {},
   "source": [
    "### 11. Performance Analysis\n",
    "\n",
    "To understand the model's performance in more detail, we'll generate a classification report \n",
    "and a confusion matrix. These will show us how well the model distinguishes between different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc309a7-fa59-4f2c-8781-31cf7c8bcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[emotion_mapping[i+1] for i in range(7)]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Log the confusion matrix as an image to TensorBoard\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[emotion_mapping[i+1] for i in range(7)],\n",
    "            yticklabels=[emotion_mapping[i+1] for i in range(7)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Convert Matplotlib plot to a tensor and add it to TensorBoard\n",
    "plt.savefig('confusion_matrix.png')\n",
    "image = plt.imread('confusion_matrix.png')\n",
    "writer.add_image('Confusion Matrix', image, 0, dataformats='HWC')\n",
    "plt.show()\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4956c-5086-44dd-ae7d-48942b6f4144",
   "metadata": {},
   "source": [
    "The classification report provides precision, recall, and F1-score for each emotion class. \n",
    "The confusion matrix visualizes the true vs. predicted labels, helping us identify any common misclassifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12599b6-b063-4ac9-aec3-92df107a3dad",
   "metadata": {},
   "source": [
    "### 12. Conclusion\n",
    "\n",
    "In this notebook, we've successfully built a facial expression recognition model using transfer learning with PyTorch. \n",
    "By leveraging a pre-trained ResNet50 model, we were able to achieve high accuracy with limited data.\n",
    "\n",
    "There are several ways we could further improve this model:\n",
    "- Experiment with different pre-trained models like VGGFace.\n",
    "- Fine-tune more layers of the pre-trained model instead of just the final layers.\n",
    "- Apply more advanced data augmentation techniques.\n",
    "\n",
    "Feel free to explore these options and see how they affect the model's performance!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7939e57b-9b83-43ac-b5e6-65483becdad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
