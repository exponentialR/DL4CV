{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d8693c-ee26-4319-ade2-13e37b679e6a",
   "metadata": {},
   "source": [
    "<img src = \"https://github.com/exponentialR/DL4CV/blob/main/media/BMC_Summer_Course_Deep_Learning_for_Computer_Vision.jpg?raw=true\" alt='BMC Summer Course' width='300'/>\n",
    "\n",
    "# BMC Summer Course: Deep Learning for Computer Vision\n",
    "## Walkthrough Excercise on ResNet-18 with Custom Dataset\n",
    "\n",
    "Author: Samuel A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe2224-b000-42c1-8dc7-af006b100deb",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "In this exercise, you will use the ResNet-18 model we built from scratch in `2.1` to classify images into three categories: `cats`, `dogs`, and `pandas`. This exercise will help you understand how to implement a deep learning model architecture using a defined architexture, prepare a dataset, train the model, and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f422797-eb08-4c37-8c80-e87f3a74e531",
   "metadata": {},
   "source": [
    "\n",
    "### **What You Will Learn:**\n",
    "- Import and Instantiating the ResNet-18 Model.\n",
    "- Preparing a custom dataset.\n",
    "- Splitting the dataset into training and validation sets.\n",
    "- Training the ResNet-18 model.\n",
    "- Evaluating model performance and visualizing results.\n",
    "- Using TensorBoard to track metrics during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b31e2d-40be-4300-9bfe-6d11537d7aae",
   "metadata": {},
   "source": [
    "## **2. Setup and Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d2927-3964-4adf-9467-d82a0376cac5",
   "metadata": {},
   "source": [
    "### **2.1 Import Libraries**\n",
    "\n",
    "First, we will import the ResNet18 code from `2.1`. We will also import all the necessary libraries. These include PyTorch for building and training the model, torchvision for handling image datasets, and TensorBoard for visualizing the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f07b93d9-c666-4724-aa55-db2dcaffb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorboard gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b98cdc-17b1-4713-9507-e1101b7d6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e332b7e-c145-483a-bf1a-8feea8ab3286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 94298    0 94298    0     0   149k      0 --:--:-- --:--:-- --:--:--  149k\n",
      "100  170k    0  170k    0     0   272k      0 --:--:-- --:--:-- --:--:--  273k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0curl: (6) Could not resolve host: requests\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://github.com/import requests\n",
    "\n",
    "resnet_url = 'https://github.com/exponentialR/DL4CV/raw/main/resnet18.py'  # Make sure to use the raw link for files\n",
    "response = requests.get(resnet_url)\n",
    "\n",
    "with open('resnet18.py', 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dac2e85-e8fe-4db2-a9b5-e93cb5e3bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the utils code\n",
    "utils_url = 'https://github.com/exponentialR/DL4CV/raw/main/utils.py'\n",
    "response = requests.get(utils_url)\n",
    "with open('utils.py', 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a483732-3273-4c19-8476-b76880232cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model architecture\n",
    "from resnet18 import ResNet18\n",
    "from utils import list_files\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import gdown\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b57c26c-45c4-4eb0-8282-73c10629ed73",
   "metadata": {},
   "source": [
    "### Check Device\n",
    "We will check if a GPU is available. if not, we will use the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c49ad77-3061-4917-a8d4-3a13005cd2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf5fef-3719-4455-ae4f-507e1f117123",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Data Preparation \n",
    "### 3.1 Download the Dataset\n",
    "Let's download the dataset and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cea7b17-2044-4d2e-965e-bfcb99ae21d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1qw-Vj5IvLIK3Uqzr2if-xSzuzIqEgEuM\n",
      "From (redirected): https://drive.google.com/uc?id=1qw-Vj5IvLIK3Uqzr2if-xSzuzIqEgEuM&confirm=t&uuid=f1b7fbbd-1908-49cb-9bc1-d7dedac05af1\n",
      "To: C:\\Users\\sadebayo\\OneDrive - Belfast Metropolitan College\\Castlereagh\\BMC-Summer Course\\Deep Learning for CV\\Day-2\\animals.zip\n",
      "100%|██████████| 197M/197M [00:05<00:00, 35.7MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'animals.zip'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://drive.google.com/uc?id=1qw-Vj5IvLIK3Uqzr2if-xSzuzIqEgEuM'\n",
    "output = 'animals.zip'\n",
    "\n",
    "gdown.download(url, output, quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ea8c192-cbda-4c3d-80b7-5b54fcb235f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('animals.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('animals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3098c-92cf-4ea9-85c1-d4730e8614a4",
   "metadata": {},
   "source": [
    "### 3.2 Dataset Structure\n",
    "The dataset should be organized into three folders: `cats`, `dogs`, and `pandas`. Each folder contains images corresponding to that category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45f1168f-7ea6-4de7-af60-402e9f8fb11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── animals/\n",
      "│   ├── cats/\n",
      "│   │   ├── cats_00001.jpg\n",
      "│   │   ├── cats_00002.jpg\n",
      "│   │   ├── cats_00003.jpg\n",
      "│   │   ├── cats_00004.jpg\n",
      "│   │   ├── cats_00005.jpg\n",
      "│   ├── dogs/\n",
      "│   │   ├── dogs_00001.jpg\n",
      "│   │   ├── dogs_00002.jpg\n",
      "│   │   ├── dogs_00003.jpg\n",
      "│   │   ├── dogs_00004.jpg\n",
      "│   │   ├── dogs_00005.jpg\n",
      "│   ├── panda/\n",
      "│   │   ├── panda_00001.jpg\n",
      "│   │   ├── panda_00002.jpg\n",
      "│   │   ├── panda_00003.jpg\n",
      "│   │   ├── panda_00004.jpg\n",
      "│   │   ├── panda_00005.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_files('animals', limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eca2e1-72b1-4adb-9029-63c2b3e2e9ff",
   "metadata": {},
   "source": [
    "### 3.3 Define Transformations\n",
    "We will resize the images to 224x224 pixels (the expected input size for ResNet-18) and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3e96003-ac1c-486f-bb88-aac5720038bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and validation datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a3e02-07f0-49a8-9756-df9b904ce631",
   "metadata": {},
   "source": [
    "### 3.4 Load and Split the Dataset\n",
    "We will load the dataset using the ImageFolder class, which automatically assigns labels based on folder names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c33ae66b-05a9-43f0-b9f7-9e23e41bc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_dir = 'animals/animals'\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split the dataset into training (80%) and validation (20%) sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader objects for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845bc09-62ed-48ce-82e5-c4c9d6db9b26",
   "metadata": {},
   "source": [
    "## Implementing ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd77755-bd9b-44f0-a4dd-63bfc0c57a10",
   "metadata": {},
   "source": [
    "### 4.1 Instantiate the Model (ResNet-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "414f836a-ee83-4c87-9068-efd9f69160dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9dfed4-0f71-45b7-a30b-58fdb9a3a026",
   "metadata": {},
   "source": [
    "### 4.2 Define Loss Function and Optimizer\n",
    "We will use Cross-Entropy Loss and the Adam optimizer for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08315641-5d0d-4ebc-b61d-da510cbec9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9759e-f6c6-4c6f-804a-26f2221e139c",
   "metadata": {},
   "source": [
    "## 5. Training the Model \n",
    "### 5.1 Set Up TensorBoard\n",
    "We will setup Tensorboard to visualize training metrics like loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3193115-73ce-4ad7-879d-15822f6a8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TensorBoard\n",
    "writer = SummaryWriter('runs/ResNet_exercise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fd38d-7ffc-4cb0-8a3c-37be3e6f7b75",
   "metadata": {},
   "source": [
    "### 5.2 Training Loop\n",
    "We will train the model for a specified number of epochs, tracking the loss and accuracy. These metrics will be logged to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a1347ca-adbb-47c6-80bc-9765e85f13a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9395, Accuracy: 57.29%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    # Log the metrics to TensorBoard\n",
    "    writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
    "    writer.add_scalar('Training Accuracy', epoch_acc, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd0873-6bab-460b-b094-402ccabafb33",
   "metadata": {},
   "source": [
    "## Evaluating the Model \n",
    "After training, we will evaluate the model on the validation dataset to measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314c2c8-9a0a-4191-8c6c-7294295a9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "\n",
    "# Log the validation accuracy\n",
    "writer.add_scalar('Validation Accuracy', val_acc, num_epochs)\n",
    "\n",
    "print(f'Accuracy of the network on the validation images: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184003b7-7696-4855-94a8-61873c414f58",
   "metadata": {},
   "source": [
    "## 7. Visualizing Results\n",
    "We will visualize some validation images along with their predicted and actual labels to see how well the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf77a0a-13fe-492f-9b59-aaac501913a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize some validation images and predictions\n",
    "def visualize_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = data_iter.next()\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plot some images with their predictions\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i in range(5):\n",
    "        axes[i].imshow(np.transpose(images[i].cpu().numpy(), (1, 2, 0)))\n",
    "        axes[i].set_title(f'Pred: {preds[i].item()}, True: {labels[i].item()}')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_predictions(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd9423-be10-4a84-8fc9-fbe924ff9ea4",
   "metadata": {},
   "source": [
    "## 8. Saving and Loading the Model\n",
    "After training, it’s important to save the model so that you can load it later for inference or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c2400-dd57-4a18-aec5-fb0fb4a7a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'resnet18_cats_dogs_pandas.pth')\n",
    "\n",
    "# To load the model\n",
    "# model.load_state_dict(torch.load('resnet18_cats_dogs_pandas.pth'))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa69e51-d82f-4a30-91f7-ff29d0c642b3",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Further Work\n",
    "### 9.1 Summary\n",
    "In this exercise, we built and trained a ResNet-18 model from scratch to classify images of cats, dogs, and pandas. We went through the entire process from implementing the architecture to data preparation, model training, evaluation, and visualization.\n",
    "\n",
    "### 9.2 Further Work\n",
    "- Data Augmentation: Experiment with different data augmentation techniques to improve the model's performance.\n",
    "- Hyperparameter Tuning: Try different learning rates, batch sizes, and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a397b-14ce-42a8-a317-c5a71595fcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
